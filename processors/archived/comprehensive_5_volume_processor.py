#!/usr/bin/env python3
"""
Comprehensive 5-Volume Processor with Character/Storyline/Timeline Integration
Optimized for efficiency while maintaining depth of analysis
"""

import os
import sys
import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass
from collections import defaultdict
import re

# Setup UTF-8 environment first
os.environ['PYTHONIOENCODING'] = 'utf-8'
sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1)
sys.stderr = open(sys.stderr.fileno(), mode='w', encoding='utf-8', buffering=1)

from deepseek_integration import DeepSeekClient, create_deepseek_config
from character_variant_resolver import CharacterVariantResolver
from enhanced_rag import EnhancedRAGSystem

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class VolumeAnalysisResult:
    """Complete analysis result for a volume"""
    volume_id: int
    volume_title: str
    processing_time: float
    tokens_used: int
    cost: float
    
    # Analysis content
    character_analysis: Dict = None
    storyline_analysis: Dict = None
    timeline_events: List = None
    volume_summary: Dict = None
    
    # Statistics
    chunks_analyzed: int = 0
    total_chunks: int = 0
    total_chars: int = 0
    
    def __post_init__(self):
        if self.character_analysis is None:
            self.character_analysis = {}
        if self.storyline_analysis is None:
            self.storyline_analysis = {}
        if self.timeline_events is None:
            self.timeline_events = []
        if self.volume_summary is None:
            self.volume_summary = {}

class ComprehensiveVolumeProcessor:
    """Comprehensive processor integrating character, storyline, and timeline analysis"""
    
    def __init__(self):
        self.deepseek_config = create_deepseek_config()
        self.deepseek_client = DeepSeekClient(self.deepseek_config)
        self.character_resolver = CharacterVariantResolver()
        
        # Try to initialize RAG system
        try:
            self.rag_system = EnhancedRAGSystem()
        except:
            self.rag_system = None
        
        self.global_stats = {
            'start_time': datetime.now(),
            'total_tokens': 0,
            'total_cost': 0.0,
            'total_deepseek_calls': 0
        }
        
        self.volume_results: List[VolumeAnalysisResult] = []
        
        # Enhanced analysis prompts
        self.analysis_prompts = self._setup_comprehensive_prompts()
        
        print("=" * 80)
        print("ğŸš€ COMPREHENSIVE 5-VOLUME PROCESSOR")
        print("=" * 80)
        print(f"ğŸ• Start time: {self.global_stats['start_time']}")
        print("ğŸ“– Target: Process volumes 1-5 with full integration")
        print("ğŸ¯ Analysis: Character, Storyline, Timeline, Thematic")
        print("âš¡ Optimized: Sample-based analysis for efficiency")
        print("=" * 80)
    
    def _setup_comprehensive_prompts(self) -> Dict[str, str]:
        """Setup comprehensive analysis prompts"""
        return {
            'volume_comprehensive_analysis': """
ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡å°è¯´åˆ†æä¸“å®¶ã€‚è¯·å¯¹ç¬¬{volume_id}å·è¿›è¡Œå…¨é¢åˆ†æã€‚

å·ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{volume_title}
- æ€»ç‰‡æ®µï¼š{total_chunks}
- æ€»å­—æ•°ï¼š{total_chars:,}

ä»£è¡¨æ€§å†…å®¹æ ·æœ¬ï¼š
{content_samples}

è¯·è¿”å›JSONæ ¼å¼çš„å…¨é¢åˆ†æï¼š
{{
  "volume_overview": {{
    "main_theme": "ä¸»è¦ä¸»é¢˜",
    "volume_tone": "æ•´å·åŸºè°ƒ",
    "narrative_style": "å™è¿°é£æ ¼",
    "significance": "åœ¨æ•´ä¸ªç³»åˆ—ä¸­çš„é‡è¦æ€§"
  }},
  "character_analysis": {{
    "main_characters": [
      {{
        "name": "è§’è‰²å",
        "variants": ["è§’è‰²åˆ«å1", "è§’è‰²åˆ«å2"],
        "role": "protagonist/antagonist/supporting",
        "personality_traits": ["ç‰¹å¾1", "ç‰¹å¾2"],
        "character_arc": "åœ¨æœ¬å·çš„å‘å±•è½¨è¿¹",
        "key_relationships": ["ä¸XXçš„å…³ç³»"],
        "growth_moments": ["æˆé•¿æ—¶åˆ»"],
        "importance_score": 0.9
      }}
    ],
    "character_relationships": [
      {{
        "character1": "è§’è‰²A",
        "character2": "è§’è‰²B",
        "relationship_type": "friend/enemy/family/romance/rivalry",
        "relationship_evolution": "å…³ç³»å‘å±•å˜åŒ–",
        "key_interactions": ["é‡è¦äº’åŠ¨"]
      }}
    ],
    "new_characters": ["æ–°ç™»åœºè§’è‰²"]
  }},
  "storyline_analysis": {{
    "plot_structure": {{
      "opening": "å¼€ç¯‡æƒ…å†µ",
      "inciting_incident": "å¼•å‘äº‹ä»¶",
      "rising_action": "æƒ…èŠ‚å‘å±•",
      "climax": "é«˜æ½®éƒ¨åˆ†",
      "falling_action": "é«˜æ½®åå‘å±•",
      "resolution": "ç»“å±€æˆ–æ‚¬å¿µ"
    }},
    "major_events": [
      {{
        "event": "é‡å¤§äº‹ä»¶æè¿°",
        "type": "battle/revelation/character_development/plot_twist",
        "significance": "é‡è¦æ€§å’Œå½±å“",
        "characters_involved": ["å‚ä¸è§’è‰²"],
        "consequences": "åæœå’Œå½±å“",
        "timeline_position": "åœ¨å·ä¸­çš„ä½ç½®"
      }}
    ],
    "plot_threads": [
      {{
        "thread_name": "æ•…äº‹çº¿åç§°",
        "status": "introduced/developed/resolved/ongoing",
        "description": "æ•…äº‹çº¿æè¿°",
        "key_developments": ["å…³é”®å‘å±•"]
      }}
    ],
    "foreshadowing": ["ä¼ç¬”å’Œé¢„ç¤º"],
    "cliffhangers": ["æ‚¬å¿µå’Œé’©å­"]
  }},
  "timeline_events": [
    {{
      "event_id": "vol{volume_id}_event_1",
      "event_description": "äº‹ä»¶æè¿°",
      "event_type": "battle/meeting/revelation/departure",
      "time_reference": "æ—¶é—´å‚è€ƒï¼ˆå¦‚æœæœ‰ï¼‰",
      "location": "åœ°ç‚¹",
      "participants": ["å‚ä¸è€…"],
      "importance": 0.8,
      "causes": ["å¯¼å› "],
      "effects": ["ç»“æœ"]
    }}
  ],
  "thematic_analysis": {{
    "central_themes": ["ä¸­å¿ƒä¸»é¢˜"],
    "symbolic_elements": ["è±¡å¾å…ƒç´ "],
    "moral_lessons": ["é“å¾·æ•™è®­"],
    "philosophical_questions": ["å“²å­¦æ€è€ƒ"],
    "cultural_elements": ["æ–‡åŒ–å…ƒç´ "]
  }},
  "connections": {{
    "previous_volume": "ä¸å‰å·çš„è”ç³»",
    "next_volume_setup": "ä¸ºä¸‹å·é“ºå«",
    "series_significance": "åœ¨æ•´ä¸ªç³»åˆ—ä¸­çš„åœ°ä½"
  }}
}}

è¯·åŸºäºæä¾›çš„å†…å®¹æ ·æœ¬è¿›è¡Œæ·±å…¥åˆ†æã€‚
""",
            
            'cross_volume_synthesis': """
åŸºäºä»¥ä¸‹5å·çš„åˆ†æç»“æœï¼Œè¯·è¿›è¡Œè·¨å·ç»¼åˆåˆ†æï¼š

{volume_analyses}

è¯·è¿”å›JSONæ ¼å¼çš„ç»¼åˆåˆ†æï¼š
{{
  "series_overview": {{
    "overall_narrative_arc": "æ•´ä½“å™äº‹å¼§çº¿",
    "central_conflict": "æ ¸å¿ƒå†²çª",
    "thematic_progression": "ä¸»é¢˜å‘å±•",
    "world_building": "ä¸–ç•Œè§‚æ„å»º"
  }},
  "character_evolution": [
    {{
      "character": "è§’è‰²å",
      "cross_volume_arc": "è·¨å·å‘å±•è½¨è¿¹",
      "key_transformation_moments": ["å…³é”®è½¬å˜æ—¶åˆ»"],
      "relationship_developments": ["å…³ç³»å‘å±•"]
    }}
  ],
  "plot_threads_tracking": [
    {{
      "thread": "æ•…äº‹çº¿",
      "volume_progression": ["åœ¨å„å·ä¸­çš„å‘å±•"],
      "resolution_status": "resolved/ongoing/abandoned"
    }}
  ],
  "timeline_summary": {{
    "major_events_chronology": ["é‡å¤§äº‹ä»¶æ—¶é—´é¡ºåº"],
    "turning_points": ["è½¬æŠ˜ç‚¹"],
    "story_pacing": "æ•…äº‹èŠ‚å¥åˆ†æ"
  }}
}}
"""
        }
    
    async def process_comprehensive_5_volumes(self) -> Dict[str, Any]:
        """Process 5 volumes with comprehensive analysis"""
        
        try:
            # Step 1: Extract volume content
            print("\nğŸ“– Step 1: Extracting volume content...")
            volume_data = await self._extract_optimized_volume_content()
            
            if not volume_data['success']:
                raise Exception(f"Volume extraction failed: {volume_data['error']}")
            
            volumes_content = volume_data['volumes_content']
            print(f"âœ… Extracted content for {len(volumes_content)} volumes")
            
            # Step 2: Process each volume comprehensively
            for volume_id in sorted(volumes_content.keys()):
                print(f"\n" + "=" * 60)
                print(f"ğŸ“š COMPREHENSIVE ANALYSIS - VOLUME {volume_id}")
                print("=" * 60)
                
                result = await self._analyze_volume_comprehensively(volume_id, volumes_content[volume_id])
                self.volume_results.append(result)
                
                # Progress update
                print(f"\nâœ… Volume {volume_id} analysis complete")
                print(f"   â±ï¸  Time: {result.processing_time:.1f}s")
                print(f"   ğŸ’° Tokens: {result.tokens_used:,} | Cost: ${result.cost:.4f}")
                print(f"   ğŸ“Š Coverage: {result.chunks_analyzed}/{result.total_chunks} chunks")
            
            # Step 3: Cross-volume synthesis
            print(f"\nğŸ“Š Step 3: Cross-volume synthesis...")
            cross_analysis = await self._perform_cross_volume_analysis()
            
            # Step 4: Generate final report
            final_report = self._generate_comprehensive_report(cross_analysis)
            
            return final_report
            
        except Exception as e:
            logger.error(f"Comprehensive processing failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _extract_optimized_volume_content(self) -> Dict[str, Any]:
        """Extract volume content with optimization for analysis"""
        try:
            from qdrant_client import QdrantClient
            client = QdrantClient(url="http://localhost:32768", verify=False)
            
            # Get all data points
            all_points = []
            offset = None
            batch_size = 1000
            
            while True:
                points = client.scroll(
                    collection_name="test_novel2",
                    limit=batch_size,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                batch_points = points[0]
                all_points.extend(batch_points)
                
                if len(batch_points) < batch_size:
                    break
                
                offset = points[1] if len(points) > 1 and points[1] else None
                if not offset:
                    break
            
            # Organize content by volume with strategic sampling
            volumes_content = defaultdict(lambda: {
                'title': '',
                'all_chunks': [],
                'sample_chunks': [],
                'total_chars': 0
            })
            
            # Patterns for content organization
            volume_patterns = [r'é­”æ³•ç¦ä¹¦ç›®å½•\s*(\d+)', r'ç¬¬(\d+)å·']
            header_patterns = [r'â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡', r'ä½œè€…ï¼šéŒæ± å’Œé©¬', r'æ’ç”»ï¼šç°æ‘æ¸…å­']
            
            for i, point in enumerate(all_points):
                content = point.payload.get('chunk', '')
                
                if not content.strip():
                    continue
                
                # Check if header
                is_header = any(re.search(pattern, content) for pattern in header_patterns)
                
                # Determine volume
                volume_num = None
                for pattern in volume_patterns:
                    match = re.search(pattern, content)
                    if match:
                        volume_num = int(match.group(1))
                        break
                
                if not volume_num:
                    estimated_volume = (i // (len(all_points) // 22)) + 1
                    if estimated_volume <= 22:
                        volume_num = estimated_volume
                
                if volume_num and volume_num <= 5:
                    if is_header and not volumes_content[volume_num]['title']:
                        # Extract title
                        lines = content.split('\n')
                        for line in lines:
                            if 'é­”æ³•ç¦ä¹¦ç›®å½•' in line and str(volume_num) in line:
                                volumes_content[volume_num]['title'] = line.strip()
                                break
                    else:
                        # Add content chunk
                        chunk_data = {
                            'content': content,
                            'length': len(content),
                            'index': len(volumes_content[volume_num]['all_chunks'])
                        }
                        
                        volumes_content[volume_num]['all_chunks'].append(chunk_data)
                        volumes_content[volume_num]['total_chars'] += len(content)
            
            # Strategic sampling for analysis
            for vol_id, vol_data in volumes_content.items():
                all_chunks = vol_data['all_chunks']
                total_chunks = len(all_chunks)
                
                if total_chunks > 20:
                    # Sample strategically: beginning, middle, end, plus random
                    sample_indices = set()
                    
                    # Beginning (first 3)
                    sample_indices.update(range(min(3, total_chunks)))
                    
                    # End (last 3)
                    sample_indices.update(range(max(0, total_chunks - 3), total_chunks))
                    
                    # Middle sections
                    middle_points = [total_chunks // 4, total_chunks // 2, 3 * total_chunks // 4]
                    sample_indices.update(middle_points)
                    
                    # Random additional samples
                    import random
                    random.seed(42)  # For reproducibility
                    additional_samples = random.sample(range(total_chunks), min(8, total_chunks - len(sample_indices)))
                    sample_indices.update(additional_samples)
                    
                    vol_data['sample_chunks'] = [all_chunks[i] for i in sorted(sample_indices)]
                else:
                    # Use all chunks if volume is small
                    vol_data['sample_chunks'] = all_chunks
            
            # Convert to regular dict
            result = {}
            for vol_id, vol_data in volumes_content.items():
                if vol_data['all_chunks']:
                    result[vol_id] = {
                        'title': vol_data['title'] or f"é­”æ³•ç¦ä¹¦ç›®å½• ç¬¬{vol_id}å·",
                        'all_chunks': vol_data['all_chunks'],
                        'sample_chunks': vol_data['sample_chunks'],
                        'total_chars': vol_data['total_chars'],
                        'total_chunks': len(vol_data['all_chunks']),
                        'sample_size': len(vol_data['sample_chunks'])
                    }
            
            print(f"   ğŸ“š Organized {len(result)} volumes")
            for vol_id, vol_data in result.items():
                print(f"      Volume {vol_id}: {vol_data['total_chunks']} chunks â†’ {vol_data['sample_size']} samples")
            
            return {
                'success': True,
                'volumes_content': result
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _analyze_volume_comprehensively(self, volume_id: int, volume_data: Dict[str, Any]) -> VolumeAnalysisResult:
        """Perform comprehensive analysis on a single volume"""
        
        start_time = time.time()
        
        print(f"ğŸ“– Volume {volume_id}: {volume_data['title']}")
        print(f"ğŸ“Š Analyzing {volume_data['sample_size']} sample chunks from {volume_data['total_chunks']} total")
        print(f"ğŸ“ Total characters: {volume_data['total_chars']:,}")
        
        # Prepare content samples for analysis
        content_samples = []
        for i, chunk in enumerate(volume_data['sample_chunks'][:15]):  # Limit to 15 samples
            sample_preview = chunk['content'][:500] + "..." if len(chunk['content']) > 500 else chunk['content']
            content_samples.append(f"æ ·æœ¬ {i+1}:\n{sample_preview}")
        
        combined_samples = "\n\n".join(content_samples)
        
        try:
            # Comprehensive analysis
            prompt = self.analysis_prompts['volume_comprehensive_analysis'].format(
                volume_id=volume_id,
                volume_title=volume_data['title'],
                total_chunks=volume_data['total_chunks'],
                total_chars=volume_data['total_chars'],
                content_samples=combined_samples
            )
            
            print(f"ğŸ”„ Running comprehensive analysis...")
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.deepseek_client.generate_character_response(
                messages=messages,
                max_tokens=3000,  # Larger token limit for comprehensive analysis
                temperature=0.2
            )
            
            if response.get("success"):
                # Calculate tokens and cost
                tokens_used = len(prompt) // 4 + len(response["response"]) // 4
                cost = tokens_used * 0.00002
                
                # Update global stats
                self.global_stats['total_tokens'] += tokens_used
                self.global_stats['total_cost'] += cost
                self.global_stats['total_deepseek_calls'] += 1
                
                # Parse response
                try:
                    response_text = response["response"]
                    if "```json" in response_text:
                        start = response_text.find("```json") + 7
                        end = response_text.find("```", start)
                        json_text = response_text[start:end].strip()
                    elif "{" in response_text:
                        start = response_text.find("{")
                        end = response_text.rfind("}") + 1
                        json_text = response_text[start:end]
                    else:
                        json_text = response_text
                    
                    analysis_data = json.loads(json_text)
                    
                    # Extract specific analysis components
                    character_analysis = analysis_data.get('character_analysis', {})
                    storyline_analysis = analysis_data.get('storyline_analysis', {})
                    timeline_events = analysis_data.get('timeline_events', [])
                    
                    # Process characters through variant resolver
                    if 'main_characters' in character_analysis:
                        for char in character_analysis['main_characters']:
                            if 'variants' in char:
                                resolved_name = self.character_resolver.resolve_character_name(char['name'])
                                char['resolved_name'] = resolved_name
                    
                    processing_time = time.time() - start_time
                    
                    result = VolumeAnalysisResult(
                        volume_id=volume_id,
                        volume_title=volume_data['title'],
                        processing_time=processing_time,
                        tokens_used=tokens_used,
                        cost=cost,
                        character_analysis=character_analysis,
                        storyline_analysis=storyline_analysis,
                        timeline_events=timeline_events,
                        volume_summary=analysis_data.get('volume_overview', {}),
                        chunks_analyzed=volume_data['sample_size'],
                        total_chunks=volume_data['total_chunks'],
                        total_chars=volume_data['total_chars']
                    )
                    
                    print(f"âœ… Analysis successful: {tokens_used:,} tokens, ${cost:.4f}")
                    return result
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSON parsing error: {e}")
                    # Return basic result with raw response
                    processing_time = time.time() - start_time
                    return VolumeAnalysisResult(
                        volume_id=volume_id,
                        volume_title=volume_data['title'],
                        processing_time=processing_time,
                        tokens_used=tokens_used,
                        cost=cost,
                        chunks_analyzed=volume_data['sample_size'],
                        total_chunks=volume_data['total_chunks'],
                        total_chars=volume_data['total_chars']
                    )
            else:
                print(f"âŒ DeepSeek error: {response.get('error', 'Unknown error')}")
                processing_time = time.time() - start_time
                return VolumeAnalysisResult(
                    volume_id=volume_id,
                    volume_title=volume_data['title'],
                    processing_time=processing_time,
                    tokens_used=0,
                    cost=0.0,
                    chunks_analyzed=0,
                    total_chunks=volume_data['total_chunks'],
                    total_chars=volume_data['total_chars']
                )
                
        except Exception as e:
            print(f"âŒ Analysis error: {e}")
            processing_time = time.time() - start_time
            return VolumeAnalysisResult(
                volume_id=volume_id,
                volume_title=volume_data['title'],
                processing_time=processing_time,
                tokens_used=0,
                cost=0.0,
                chunks_analyzed=0,
                total_chunks=volume_data['total_chunks'],
                total_chars=volume_data['total_chars']
            )
    
    async def _perform_cross_volume_analysis(self) -> Dict[str, Any]:
        """Perform cross-volume synthesis analysis"""
        
        print(f"ğŸ”„ Performing cross-volume synthesis...")
        
        try:
            # Prepare volume analyses for synthesis
            volume_summaries = []
            for result in self.volume_results:
                summary = {
                    'volume_id': result.volume_id,
                    'title': result.volume_title,
                    'character_analysis': result.character_analysis,
                    'storyline_analysis': result.storyline_analysis,
                    'timeline_events': result.timeline_events,
                    'volume_summary': result.volume_summary
                }
                volume_summaries.append(summary)
            
            analyses_text = json.dumps(volume_summaries, ensure_ascii=False, indent=2)
            
            prompt = self.analysis_prompts['cross_volume_synthesis'].format(
                volume_analyses=analyses_text
            )
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.deepseek_client.generate_character_response(
                messages=messages,
                max_tokens=2000,
                temperature=0.2
            )
            
            if response.get("success"):
                tokens_used = len(prompt) // 4 + len(response["response"]) // 4
                cost = tokens_used * 0.00002
                
                self.global_stats['total_tokens'] += tokens_used
                self.global_stats['total_cost'] += cost
                self.global_stats['total_deepseek_calls'] += 1
                
                try:
                    response_text = response["response"]
                    if "```json" in response_text:
                        start = response_text.find("```json") + 7
                        end = response_text.find("```", start)
                        json_text = response_text[start:end].strip()
                    elif "{" in response_text:
                        start = response_text.find("{")
                        end = response_text.rfind("}") + 1
                        json_text = response_text[start:end]
                    else:
                        json_text = response_text
                    
                    cross_analysis = json.loads(json_text)
                    print(f"âœ… Cross-volume analysis: {tokens_used:,} tokens, ${cost:.4f}")
                    return cross_analysis
                    
                except json.JSONDecodeError:
                    print(f"âŒ Cross-volume analysis JSON parse error")
                    return {}
            else:
                print(f"âŒ Cross-volume analysis failed")
                return {}
                
        except Exception as e:
            print(f"âŒ Cross-volume analysis error: {e}")
            return {}
    
    def _generate_comprehensive_report(self, cross_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        
        end_time = datetime.now()
        total_time = (end_time - self.global_stats['start_time']).total_seconds()
        
        # Aggregate statistics
        total_chunks_analyzed = sum(result.chunks_analyzed for result in self.volume_results)
        total_chunks_available = sum(result.total_chunks for result in self.volume_results)
        total_chars = sum(result.total_chars for result in self.volume_results)
        successful_volumes = len([r for r in self.volume_results if r.tokens_used > 0])
        
        report = {
            'processing_metadata': {
                'start_time': self.global_stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'total_processing_time': f"{total_time:.1f} seconds",
                'total_processing_minutes': f"{total_time/60:.1f} minutes",
                'volumes_processed': len(self.volume_results),
                'successful_volumes': successful_volumes,
                'total_chunks_analyzed': total_chunks_analyzed,
                'total_chunks_available': total_chunks_available,
                'total_characters': total_chars,
                'analysis_coverage': f"{total_chunks_analyzed/max(total_chunks_available, 1)*100:.1f}%"
            },
            'token_and_cost_analysis': {
                'total_tokens_used': self.global_stats['total_tokens'],
                'total_cost': f"${self.global_stats['total_cost']:.4f}",
                'total_deepseek_calls': self.global_stats['total_deepseek_calls'],
                'average_tokens_per_volume': self.global_stats['total_tokens'] // max(successful_volumes, 1),
                'cost_per_1000_chars': f"${(self.global_stats['total_cost'] / max(total_chars, 1)) * 1000:.4f}",
                'tokens_per_second': self.global_stats['total_tokens'] / max(total_time, 1)
            },
            'volume_analyses': [
                {
                    'volume_id': result.volume_id,
                    'volume_title': result.volume_title,
                    'processing_time': f"{result.processing_time:.1f}s",
                    'tokens_used': result.tokens_used,
                    'cost': f"${result.cost:.4f}",
                    'chunks_analyzed': result.chunks_analyzed,
                    'total_chunks': result.total_chunks,
                    'coverage': f"{result.chunks_analyzed/max(result.total_chunks, 1)*100:.1f}%",
                    'character_analysis': result.character_analysis,
                    'storyline_analysis': result.storyline_analysis,
                    'timeline_events': result.timeline_events,
                    'volume_summary': result.volume_summary
                }
                for result in self.volume_results
            ],
            'cross_volume_analysis': cross_analysis,
            'character_system_integration': {
                'character_resolver_used': True,
                'variants_resolved': True,
                'cross_volume_tracking': True
            },
            'timeline_integration': {
                'events_extracted': sum(len(result.timeline_events) for result in self.volume_results),
                'chronological_analysis': True
            },
            'success': successful_volumes == len(self.volume_results)
        }
        
        return report
    
    def print_comprehensive_report(self, report: Dict[str, Any]):
        """Print comprehensive analysis report"""
        
        print("\n" + "=" * 80)
        print("ğŸ“Š COMPREHENSIVE 5-VOLUME ANALYSIS REPORT")
        print("=" * 80)
        
        # Processing metadata
        meta = report['processing_metadata']
        print(f"\nâ±ï¸  PROCESSING SUMMARY:")
        print(f"   ğŸ• Start: {meta['start_time']}")
        print(f"   ğŸ• End: {meta['end_time']}")
        print(f"   â±ï¸  Total time: {meta['total_processing_time']} ({meta['total_processing_minutes']})")
        print(f"   ğŸ“š Volumes processed: {meta['successful_volumes']}/{meta['volumes_processed']}")
        print(f"   ğŸ“„ Chunks analyzed: {meta['total_chunks_analyzed']}/{meta['total_chunks_available']}")
        print(f"   ğŸ“Š Analysis coverage: {meta['analysis_coverage']}")
        print(f"   ğŸ“ Characters processed: {meta['total_characters']:,}")
        
        # Token and cost analysis
        tokens = report['token_and_cost_analysis']
        print(f"\nğŸ’° TOKEN USAGE & COSTS:")
        print(f"   ğŸ”¢ Total tokens: {tokens['total_tokens_used']:,}")
        print(f"   ğŸ’µ Total cost: {tokens['total_cost']}")
        print(f"   ğŸ“ API calls: {tokens['total_deepseek_calls']}")
        print(f"   ğŸ“– Avg tokens/volume: {tokens['average_tokens_per_volume']}")
        print(f"   ğŸ’² Cost per 1000 chars: {tokens['cost_per_1000_chars']}")
        print(f"   âš¡ Tokens/second: {tokens['tokens_per_second']:.1f}")
        
        # Volume breakdown
        print(f"\nğŸ“š VOLUME-BY-VOLUME ANALYSIS:")
        for vol in report['volume_analyses']:
            print(f"\n   ğŸ“– Volume {vol['volume_id']}: {vol['volume_title']}")
            print(f"      â±ï¸  Time: {vol['processing_time']}")
            print(f"      ğŸ’° Tokens: {vol['tokens_used']:,} | Cost: {vol['cost']}")
            print(f"      ğŸ“Š Coverage: {vol['coverage']} ({vol['chunks_analyzed']}/{vol['total_chunks']} chunks)")
            
            # Show character analysis summary
            if vol['character_analysis'] and 'main_characters' in vol['character_analysis']:
                char_count = len(vol['character_analysis']['main_characters'])
                print(f"      ğŸ‘¥ Characters analyzed: {char_count}")
            
            # Show timeline events
            if vol['timeline_events']:
                print(f"      ğŸ“… Timeline events: {len(vol['timeline_events'])}")
        
        # Cross-volume analysis
        cross = report['cross_volume_analysis']
        if cross:
            print(f"\nğŸ”— CROSS-VOLUME SYNTHESIS:")
            
            if 'character_evolution' in cross:
                char_evol_count = len(cross['character_evolution'])
                print(f"   ğŸ‘¥ Character evolution tracked: {char_evol_count} characters")
            
            if 'plot_threads_tracking' in cross:
                thread_count = len(cross['plot_threads_tracking'])
                print(f"   ğŸ“– Plot threads tracked: {thread_count} threads")
        
        # Integration status
        char_integration = report['character_system_integration']
        timeline_integration = report['timeline_integration']
        
        print(f"\nğŸ”§ SYSTEM INTEGRATION:")
        print(f"   âœ… Character variant resolver: {char_integration['character_resolver_used']}")
        print(f"   âœ… Cross-volume character tracking: {char_integration['cross_volume_tracking']}")
        print(f"   âœ… Timeline events extracted: {timeline_integration['events_extracted']}")
        print(f"   âœ… Chronological analysis: {timeline_integration['chronological_analysis']}")
        
        # Final status
        status = "âœ… SUCCESS" if report['success'] else "âš ï¸  PARTIAL SUCCESS"
        print(f"\nğŸ FINAL STATUS: {status}")
        print("=" * 80)

async def main():
    """Main execution function"""
    
    processor = ComprehensiveVolumeProcessor()
    
    try:
        # Process 5 volumes comprehensively
        report = await processor.process_comprehensive_5_volumes()
        
        # Print comprehensive report
        processor.print_comprehensive_report(report)
        
        # Save report to file
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"comprehensive_5_volume_analysis_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json_report = json.loads(json.dumps(report, default=str, ensure_ascii=False))
            json.dump(json_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Comprehensive analysis saved to: {report_file}")
        
    except Exception as e:
        print(f"âŒ Processing failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Cleanup
        if processor.deepseek_client.session:
            await processor.deepseek_client.close()

if __name__ == "__main__":
    asyncio.run(main())