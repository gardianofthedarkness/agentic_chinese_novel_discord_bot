#!/usr/bin/env python3
"""
Enhanced Hierarchical Novel Processor
Integrates hierarchical chapter parsing with existing agentic analysis

This processor properly handles:
1. Âç∑ (volumes/big chapters) vs Á´† (small chapters) distinction
2. Hierarchical analysis from volume level down to chapter level
3. Cross-volume character tracking and storyline continuity
4. Volume-specific thematic analysis
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass, asdict

from hierarchical_chapter_parser import HierarchicalChapterParser, ChapterNode, VolumeStructure
from simple_rag import create_rag_client
from deepseek_integration import DeepSeekClient, create_deepseek_config

logger = logging.getLogger(__name__)

@dataclass
class VolumeAnalysis:
    """Complete analysis of a volume"""
    volume_id: int
    volume_title: str
    chapter_analyses: List[Dict[str, Any]]
    volume_themes: List[str]
    character_introductions: List[str]
    character_developments: List[str]
    storyline_progression: List[str]
    volume_summary: str
    cross_chapter_connections: List[Dict[str, Any]]
    
@dataclass
class HierarchicalNovelAnalysis:
    """Complete hierarchical analysis of the novel"""
    novel_title: str
    total_volumes: int
    total_chapters: int
    volume_analyses: Dict[int, VolumeAnalysis]
    cross_volume_storylines: List[Dict[str, Any]]
    character_evolution_timeline: List[Dict[str, Any]]
    thematic_progression: List[str]
    overall_narrative_arc: str

class EnhancedHierarchicalProcessor:
    """Enhanced processor with proper Âç∑/Á´† hierarchy support"""
    
    def __init__(self, 
                 qdrant_url: str = "http://localhost:32768",
                 db_url: str = "postgresql://admin:admin@localhost:5432/novel_sim"):
        
        self.hierarchical_parser = HierarchicalChapterParser()
        self.rag_client = create_rag_client(qdrant_url)
        self.deepseek_config = create_deepseek_config()
        self.deepseek_client = DeepSeekClient(self.deepseek_config)
        
        # Analysis storage
        self.novel_structure: Dict[int, VolumeStructure] = {}
        self.volume_analyses: Dict[int, VolumeAnalysis] = {}
        self.character_timeline: List[Dict[str, Any]] = []
        self.cross_volume_storylines: List[Dict[str, Any]] = []
        
        # Enhanced prompts for hierarchical analysis
        self.prompts = self._setup_hierarchical_prompts()
        
        logger.info("Enhanced Hierarchical Processor initialized")
    
    def _setup_hierarchical_prompts(self) -> Dict[str, str]:
        """Setup prompts specifically for hierarchical analysis"""
        return {
            'volume_analysis': """
‰Ω†ÊòØ‰∏ì‰∏öÁöÑ‰∏≠ÊñáÂ∞èËØ¥ÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑ÂØπ‰ª•‰∏ãÂç∑(volume)ËøõË°åÂÖ®Èù¢ÂàÜÊûê„ÄÇ

Âç∑‰ø°ÊÅØÔºö
- Âç∑Âè∑Ôºö{volume_id}
- Âç∑Ê†áÈ¢òÔºö{volume_title}
- ÂåÖÂê´Á´†ËäÇÔºö{chapter_count}‰∏™Á´†ËäÇ
- ÊÄªÂ≠óÊï∞Ôºö{word_count}Â≠ó

ËØ∑Êåâ‰ª•‰∏ãJSONÊ†ºÂºèÂàÜÊûêËøô‰∏ÄÂç∑ÁöÑÂÜÖÂÆπÔºö
{{
  "volume_themes": ["‰∏ªÈ¢ò1", "‰∏ªÈ¢ò2", "‰∏ªÈ¢ò3"],
  "character_introductions": ["Êñ∞ËßíËâ≤1ÁöÑ‰ªãÁªç", "Êñ∞ËßíËâ≤2ÁöÑ‰ªãÁªç"],
  "character_developments": ["ËßíËâ≤AÁöÑÂèëÂ±ï", "ËßíËâ≤BÁöÑÂèòÂåñ"],
  "storyline_progression": ["ÊÉÖËäÇÁ∫ø1ÁöÑÂèëÂ±ï", "ÊÉÖËäÇÁ∫ø2ÁöÑÊé®Ëøõ"],
  "volume_summary": "Ëøô‰∏ÄÂç∑ÁöÑÊï¥‰ΩìÊ¶ÇËø∞(200-300Â≠ó)",
  "cross_chapter_connections": [
    {{
      "from_chapter": 1,
      "to_chapter": 3,
      "connection_type": "Âõ†ÊûúÂÖ≥Á≥ª",
      "description": "ËøûÊé•ÊèèËø∞"
    }}
  ]
}}

Âç∑ÂÜÖÂÆπÔºö
{volume_content}
""",

            'chapter_in_volume': """
‰Ω†ÊòØ‰∏ì‰∏öÁöÑ‰∏≠ÊñáÂ∞èËØ¥ÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑ÂàÜÊûê‰ª•‰∏ãÁ´†ËäÇÔºåÊ≥®ÊÑèÂÆÉÂú®Êï¥Âç∑‰∏≠ÁöÑ‰ΩçÁΩÆÂíå‰ΩúÁî®„ÄÇ

Á´†ËäÇ‰ø°ÊÅØÔºö
- ÊâÄÂ±ûÂç∑ÔºöÁ¨¨{volume_id}Âç∑ {volume_title}
- Á´†ËäÇÔºöÁ¨¨{chapter_id}Á´† {chapter_title}
- Âú®Âç∑‰∏≠‰ΩçÁΩÆÔºöÁ¨¨{chapter_position}/{total_chapters}Á´†

ËØ∑Êåâ‰ª•‰∏ãJSONÊ†ºÂºèÂàÜÊûêÔºö
{{
  "chapter_summary": {{
    "main_events": ["‰∏ªË¶Å‰∫ã‰ª∂1", "‰∏ªË¶Å‰∫ã‰ª∂2"],
    "characters_involved": ["ËßíËâ≤1", "ËßíËâ≤2"],
    "emotional_arc": "ÊÉÖÊÑüÂèëÂ±ïËΩ®Ëøπ",
    "plot_significance": 0.8,
    "volume_role": "Âú®Êï¥Âç∑‰∏≠ÁöÑ‰ΩúÁî®"
  }},
  "character_analysis": [
    {{
      "name": "ËßíËâ≤Âêç",
      "role_in_chapter": "Âú®Êú¨Á´†ÁöÑ‰ΩúÁî®",
      "development": "ÂèëÂ±ïÂèòÂåñ",
      "relationships": {{"ÂÖ∂‰ªñËßíËâ≤": "ÂÖ≥Á≥ªÂèòÂåñ"}}
    }}
  ],
  "storyline_threads": [
    {{
      "thread_title": "ÊïÖ‰∫ãÁ∫øÂêçÁß∞",
      "development": "Âú®Êú¨Á´†ÁöÑÂèëÂ±ï",
      "connects_to": ["ÂâçÈù¢Á´†ËäÇ", "ÂêéÁª≠Á´†ËäÇ"],
      "importance": 0.7
    }}
  ]
}}

Á´†ËäÇÂÜÖÂÆπÔºö
{content}
""",

            'cross_volume_analysis': """
Âü∫‰∫éÂ∑≤ÂàÜÊûêÁöÑÂ§ö‰∏™Âç∑ÔºåËØ∑ÂàÜÊûêË∑®Âç∑ÁöÑÊïÖ‰∫ãÂèëÂ±ïÔºö

Â∑≤ÂàÜÊûêÂç∑Ôºö{volume_summaries}

ËØ∑Êåâ‰ª•‰∏ãJSONÊ†ºÂºèÂàÜÊûêÔºö
{{
  "cross_volume_storylines": [
    {{
      "storyline_title": "Ë∑®Âç∑ÊïÖ‰∫ãÁ∫øÂêçÁß∞",
      "volume_span": [1, 2, 3],
      "development_stages": ["Á¨¨‰∏ÄÂç∑ÁöÑÂèëÂ±ï", "Á¨¨‰∫åÂç∑ÁöÑÊé®Ëøõ", "Á¨¨‰∏âÂç∑ÁöÑÈ´òÊΩÆ"],
      "key_characters": ["‰∏ªË¶ÅËßíËâ≤"],
      "resolution_status": "ongoing/resolved"
    }}
  ],
  "character_evolution": [
    {{
      "character_name": "ËßíËâ≤Âêç",
      "evolution_timeline": [
        {{"volume": 1, "state": "ÂàùÂßãÁä∂ÊÄÅ", "key_events": ["ÈáçË¶Å‰∫ã‰ª∂"]}},
        {{"volume": 2, "state": "ÂèëÂ±ïÁä∂ÊÄÅ", "key_events": ["ÈáçË¶Å‰∫ã‰ª∂"]}}
      ]
    }}
  ],
  "thematic_progression": ["‰∏ªÈ¢òÂú®ÂêÑÂç∑‰∏≠ÁöÑÊºîËøõ"],
  "narrative_arc": "Êï¥‰ΩìÂèô‰∫ãÂºßÁ∫øÁöÑÂèëÂ±ï"
}}
""",

            'volume_comparison': """
ËØ∑ÊØîËæÉÂàÜÊûê‰ª•‰∏ã‰∏§‰∏™Âç∑ÁöÑÂºÇÂêåÔºö

Á¨¨{volume1_id}Âç∑Ôºö{volume1_summary}
Á¨¨{volume2_id}Âç∑Ôºö{volume2_summary}

ËØ∑ÂàÜÊûêÔºö
1. ‰∏ªÈ¢òÁöÑÂª∂Áª≠‰∏éÂèòÂåñ
2. ËßíËâ≤ÂèëÂ±ïÁöÑÂØπÊØî
3. ÊïÖ‰∫ãËäÇÂ•èÁöÑÂ∑ÆÂºÇ
4. Âèô‰∫ãÈ£éÊ†ºÁöÑÂèòÂåñ
5. ‰∏§Âç∑‰πãÈó¥ÁöÑËøûÊé•ÁÇπ

ËØ∑Áî®‰∏≠ÊñáÂõûÁ≠îÔºåÈáçÁÇπÂÖ≥Ê≥®ËøûÁª≠ÊÄßÂíåÂèëÂ±ïÂèòÂåñ„ÄÇ
"""
        }
    
    async def process_novel_hierarchically(self, max_volumes: int = 3) -> HierarchicalNovelAnalysis:
        """Process novel with full hierarchical analysis"""
        logger.info(f"Starting hierarchical processing of up to {max_volumes} volumes...")
        
        try:
            # Step 1: Parse hierarchical structure
            structure_result = await self._parse_novel_structure()
            if not structure_result['success']:
                raise Exception(f"Structure parsing failed: {structure_result['error']}")
            
            self.novel_structure = structure_result['structure']
            
            # Step 2: Analyze each volume comprehensively
            for volume_id in sorted(self.novel_structure.keys())[:max_volumes]:
                if volume_id == 0:  # Skip unstructured chapters for now
                    continue
                    
                logger.info(f"\nProcessing Volume {volume_id}...")
                volume_analysis = await self._analyze_volume_comprehensive(volume_id)
                self.volume_analyses[volume_id] = volume_analysis
            
            # Step 3: Cross-volume analysis
            logger.info(f"\nPerforming cross-volume analysis...")
            await self._analyze_cross_volume_connections()
            
            # Step 4: Generate complete hierarchical analysis
            hierarchical_analysis = HierarchicalNovelAnalysis(
                novel_title="Unknown Novel",  # Can be detected from content
                total_volumes=len(self.novel_structure),
                total_chapters=sum(len(vol.chapters) for vol in self.novel_structure.values()),
                volume_analyses=self.volume_analyses,
                cross_volume_storylines=self.cross_volume_storylines,
                character_evolution_timeline=self.character_timeline,
                thematic_progression=[],
                overall_narrative_arc=""
            )
            
            return hierarchical_analysis
            
        except Exception as e:
            logger.error(f"Hierarchical processing error: {e}")
            raise
    
    async def _parse_novel_structure(self) -> Dict[str, Any]:
        """Parse the novel structure from Qdrant"""
        try:
            from qdrant_client import QdrantClient
            client = QdrantClient(url="http://localhost:32768", verify=False)
            
            # Get all content
            points = client.scroll(
                collection_name="test_novel2",
                limit=200,
                with_payload=True,
                with_vectors=False
            )
            
            # Combine content
            combined_content = ""
            for point in points[0]:
                if 'chunk' in point.payload:
                    combined_content += point.payload['chunk'] + "\n\n"
            
            # Parse hierarchical structure
            chapters = self.hierarchical_parser.parse_content_hierarchy(combined_content)
            
            return {
                'success': True,
                'structure': self.hierarchical_parser.parsed_structure,
                'chapters': chapters
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'structure': {},
                'chapters': []
            }
    
    async def _analyze_volume_comprehensive(self, volume_id: int) -> VolumeAnalysis:
        """Comprehensive analysis of a single volume"""
        volume = self.novel_structure[volume_id]
        logger.info(f"Analyzing Volume {volume_id}: {volume.volume_title}")
        
        # Combine all chapters in the volume
        volume_content = ""
        chapter_analyses = []
        
        for i, chapter in enumerate(volume.chapters):
            # Analyze individual chapter
            chapter_analysis = await self._analyze_chapter_in_volume(
                volume_id, chapter, i + 1, len(volume.chapters)
            )
            chapter_analyses.append(chapter_analysis)
            volume_content += chapter.content + "\n\n"
        
        # Analyze entire volume
        volume_analysis_result = await self._analyze_volume_as_whole(
            volume_id, volume.volume_title, volume_content, len(volume.chapters), volume.total_words
        )
        
        # Create VolumeAnalysis object
        volume_analysis = VolumeAnalysis(
            volume_id=volume_id,
            volume_title=volume.volume_title,
            chapter_analyses=chapter_analyses,
            volume_themes=volume_analysis_result.get('volume_themes', []),
            character_introductions=volume_analysis_result.get('character_introductions', []),
            character_developments=volume_analysis_result.get('character_developments', []),
            storyline_progression=volume_analysis_result.get('storyline_progression', []),
            volume_summary=volume_analysis_result.get('volume_summary', ''),
            cross_chapter_connections=volume_analysis_result.get('cross_chapter_connections', [])
        )
        
        return volume_analysis
    
    async def _analyze_chapter_in_volume(self, volume_id: int, chapter: ChapterNode, 
                                       chapter_position: int, total_chapters: int) -> Dict[str, Any]:
        """Analyze a single chapter within its volume context"""
        try:
            prompt = self.prompts['chapter_in_volume'].format(
                volume_id=volume_id,
                volume_title=chapter.volume_title or f"Á¨¨{volume_id}Âç∑",
                chapter_id=chapter.chapter_id,
                chapter_title=chapter.chapter_title,
                chapter_position=chapter_position,
                total_chapters=total_chapters,
                content=chapter.content[:2000]  # Limit content for analysis
            )
            
            response = await self._query_deepseek(prompt)
            if response:
                try:
                    return json.loads(self._extract_json_from_markdown(response))
                except json.JSONDecodeError:
                    return {'error': 'Failed to parse chapter analysis JSON'}
            else:
                return {'error': 'No response from DeepSeek'}
                
        except Exception as e:
            logger.error(f"Error analyzing chapter {chapter.hierarchy_id}: {e}")
            return {'error': str(e)}
    
    async def _analyze_volume_as_whole(self, volume_id: int, volume_title: str, 
                                     volume_content: str, chapter_count: int, word_count: int) -> Dict[str, Any]:
        """Analyze entire volume as a coherent unit"""
        try:
            prompt = self.prompts['volume_analysis'].format(
                volume_id=volume_id,
                volume_title=volume_title,
                chapter_count=chapter_count,
                word_count=word_count,
                volume_content=volume_content[:3000]  # Limit for analysis
            )
            
            response = await self._query_deepseek(prompt)
            if response:
                try:
                    return json.loads(self._extract_json_from_markdown(response))
                except json.JSONDecodeError:
                    return {
                        'volume_themes': [],
                        'character_introductions': [],
                        'character_developments': [],
                        'storyline_progression': [],
                        'volume_summary': response[:500],
                        'cross_chapter_connections': []
                    }
            else:
                return {}
                
        except Exception as e:
            logger.error(f"Error analyzing volume {volume_id}: {e}")
            return {}
    
    async def _analyze_cross_volume_connections(self):
        """Analyze connections and storylines across volumes"""
        if len(self.volume_analyses) < 2:
            return
        
        try:
            # Prepare volume summaries for analysis
            volume_summaries = {}
            for vol_id, analysis in self.volume_analyses.items():
                volume_summaries[vol_id] = {
                    'title': analysis.volume_title,
                    'themes': analysis.volume_themes,
                    'summary': analysis.volume_summary,
                    'characters': analysis.character_introductions + analysis.character_developments
                }
            
            prompt = self.prompts['cross_volume_analysis'].format(
                volume_summaries=json.dumps(volume_summaries, ensure_ascii=False, indent=2)
            )
            
            response = await self._query_deepseek(prompt)
            if response:
                try:
                    cross_analysis = json.loads(self._extract_json_from_markdown(response))
                    self.cross_volume_storylines = cross_analysis.get('cross_volume_storylines', [])
                    self.character_timeline = cross_analysis.get('character_evolution', [])
                except json.JSONDecodeError:
                    logger.warning("Failed to parse cross-volume analysis JSON")
                    
        except Exception as e:
            logger.error(f"Error in cross-volume analysis: {e}")
    
    async def _query_deepseek(self, prompt: str) -> Optional[str]:
        """Query DeepSeek AI with error handling"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = await self.deepseek_client.generate_character_response(
                messages=messages,
                max_tokens=2000,
                temperature=0.2
            )
            
            if response.get("success"):
                return response["response"]
            else:
                logger.error(f"DeepSeek query failed: {response.get('error')}")
                return None
                
        except Exception as e:
            logger.error(f"DeepSeek query error: {e}")
            return None
    
    def _extract_json_from_markdown(self, response_text: str) -> str:
        """Extract JSON from markdown code blocks"""
        import re
        
        # Try to find JSON in markdown code blocks
        markdown_pattern = r'```(?:json)?\\s*\\n(.*?)\\n```'
        match = re.search(markdown_pattern, response_text, re.DOTALL | re.MULTILINE)
        
        if match:
            return match.group(1).strip()
        
        # Try to find balanced braces
        start_idx = response_text.find('{')
        if start_idx != -1:
            brace_count = 0
            for i, char in enumerate(response_text[start_idx:], start_idx):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        return response_text[start_idx:i+1]
        
        return response_text.strip()
    
    def print_hierarchical_results(self):
        """Print comprehensive hierarchical analysis results"""
        print("\\n" + "="*80)
        print("HIERARCHICAL NOVEL ANALYSIS RESULTS")
        print("="*80)
        
        # Overall structure
        print(f"\\nSTRUCTURE OVERVIEW:")
        print(f"  Total Volumes: {len(self.novel_structure)}")
        print(f"  Total Chapters: {sum(len(vol.chapters) for vol in self.novel_structure.values())}")
        print(f"  Total Words: {sum(vol.total_words for vol in self.novel_structure.values()):,}")
        
        # Volume-by-volume analysis
        for vol_id in sorted(self.volume_analyses.keys()):
            analysis = self.volume_analyses[vol_id]
            print(f"\\n{'='*60}")
            print(f"Á¨¨{vol_id}Âç∑: {analysis.volume_title}")
            print("="*60)
            
            print(f"\\nüìñ VOLUME SUMMARY:")
            print(f"  {analysis.volume_summary}")
            
            print(f"\\nüé≠ MAIN THEMES:")
            for theme in analysis.volume_themes:
                print(f"  ‚Ä¢ {theme}")
            
            print(f"\\nüë• CHARACTER DEVELOPMENTS:")
            for dev in analysis.character_developments[:3]:
                print(f"  ‚Ä¢ {dev}")
            
            print(f"\\nüìö STORYLINE PROGRESSION:")
            for story in analysis.storyline_progression[:3]:
                print(f"  ‚Ä¢ {story}")
            
            print(f"\\nüîó CROSS-CHAPTER CONNECTIONS:")
            for conn in analysis.cross_chapter_connections[:2]:
                print(f"  ‚Ä¢ Ch.{conn.get('from_chapter')} ‚Üí Ch.{conn.get('to_chapter')}: {conn.get('description', '')}")
        
        # Cross-volume analysis
        if self.cross_volume_storylines:
            print(f"\\n{'='*60}")
            print("CROSS-VOLUME STORYLINES")
            print("="*60)
            
            for storyline in self.cross_volume_storylines:
                print(f"\\nüìñ {storyline.get('storyline_title', 'Unknown Storyline')}")
                print(f"   Spans volumes: {storyline.get('volume_span', [])}")
                print(f"   Status: {storyline.get('resolution_status', 'unknown')}")
                print(f"   Key characters: {', '.join(storyline.get('key_characters', []))}")
        
        # Character evolution timeline
        if self.character_timeline:
            print(f"\\n{'='*60}")
            print("CHARACTER EVOLUTION TIMELINE")
            print("="*60)
            
            for char_evo in self.character_timeline:
                print(f"\\nüë§ {char_evo.get('character_name', 'Unknown Character')}")
                for stage in char_evo.get('evolution_timeline', []):
                    vol = stage.get('volume', 0)
                    state = stage.get('state', '')
                    print(f"   Á¨¨{vol}Âç∑: {state}")

# Main execution
async def main():
    """Run enhanced hierarchical processing"""
    print("ENHANCED HIERARCHICAL NOVEL PROCESSING")
    print("="*80)
    print("Processing with proper Âç∑/Á´† hierarchy:")
    print("  ‚Ä¢ Volume-level thematic analysis")
    print("  ‚Ä¢ Chapter-in-volume contextual analysis") 
    print("  ‚Ä¢ Cross-volume storyline tracking")
    print("  ‚Ä¢ Character evolution across volumes")
    print("="*80)
    
    processor = EnhancedHierarchicalProcessor()
    
    try:
        # Process novel hierarchically
        analysis = await processor.process_novel_hierarchically(max_volumes=3)
        
        # Print results
        processor.print_hierarchical_results()
        
        # Print structure analysis
        processor.hierarchical_parser.print_structure_analysis()
        
    except Exception as e:
        print(f"Processing failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Clean up
        if processor.deepseek_client.session:
            await processor.deepseek_client.close()

if __name__ == "__main__":
    asyncio.run(main())