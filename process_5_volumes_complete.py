#!/usr/bin/env python3
"""
Process First 5 Volumes from Complete Dataset (2588 points)
With comprehensive progress monitoring and token counting
"""

import os
import sys
import asyncio
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass

# Setup UTF-8 environment first
os.environ['PYTHONIOENCODING'] = 'utf-8'
sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1)
sys.stderr = open(sys.stderr.fileno(), mode='w', encoding='utf-8', buffering=1)

from hierarchical_chapter_parser import HierarchicalChapterParser
from deepseek_integration import DeepSeekClient, create_deepseek_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class VolumeProcessingStats:
    """Track processing statistics for each volume"""
    volume_id: int
    volume_title: str
    chapters_count: int
    start_time: datetime
    end_time: datetime = None
    processing_time: float = 0.0
    chapters_processed: int = 0
    chapters_successful: int = 0
    total_tokens: int = 0
    total_cost: float = 0.0
    deepseek_calls: int = 0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []

class Complete5VolumeProcessor:
    """Process first 5 volumes from complete dataset with detailed monitoring"""
    
    def __init__(self):
        self.parser = HierarchicalChapterParser()
        self.deepseek_config = create_deepseek_config()
        self.deepseek_client = DeepSeekClient(self.deepseek_config)
        
        self.global_stats = {
            'start_time': datetime.now(),
            'volumes_stats': [],
            'total_tokens': 0,
            'total_cost': 0.0,
            'total_deepseek_calls': 0,
            'total_processing_time': 0.0
        }
        
        # Analysis prompts
        self.analysis_prompts = self._setup_prompts()
        
        print("=" * 80)
        print("üöÄ COMPLETE 5-VOLUME PROCESSING FROM FULL DATASET")
        print("=" * 80)
        print(f"üïê Start time: {self.global_stats['start_time']}")
        print("üìñ Target: Process volumes 1-5 from complete 22-volume dataset")
        print("üí∞ Monitor: Token usage, costs, and processing time")
        print("üéØ Goal: All 5 volumes with comprehensive AI analysis")
        print("=" * 80)
    
    def _setup_prompts(self) -> Dict[str, str]:
        """Setup analysis prompts"""
        return {
            'chapter_analysis': """
‰Ω†ÊòØ‰∏ì‰∏öÁöÑ‰∏≠ÊñáÂ∞èËØ¥ÂàÜÊûê‰∏ìÂÆ∂„ÄÇËØ∑ÂàÜÊûê‰ª•‰∏ãÁ´†ËäÇÔºö

Á´†ËäÇ‰ø°ÊÅØÔºö
- Á¨¨{volume_id}Âç∑Á¨¨{chapter_id}Á´†
- Ê†áÈ¢òÔºö{chapter_title}
- Â≠óÊï∞Ôºö{word_count}

ËØ∑ËøîÂõûJSONÊ†ºÂºèÂàÜÊûêÔºö
{{
  "characters": [
    {{
      "name": "ËßíËâ≤Âêç",
      "role": "‰∏ªËßí/ÈÖçËßí/ÂèçÊ¥æ",
      "actions": ["Ë°åÂä®1", "Ë°åÂä®2"],
      "development": "ÂèëÂ±ïÂèòÂåñ",
      "importance": 0.8
    }}
  ],
  "plot_events": [
    {{
      "event": "‰∫ã‰ª∂ÊèèËø∞",
      "type": "conflict/resolution/development",
      "importance": 0.8,
      "consequence": "ÂΩ±Âìç"
    }}
  ],
  "themes": ["‰∏ªÈ¢ò1", "‰∏ªÈ¢ò2"],
  "mood": "Á´†ËäÇÊ∞õÂõ¥",
  "conflicts": ["ÂÜ≤Á™Å1", "ÂÜ≤Á™Å2"],
  "summary": "Á´†ËäÇÊ†∏ÂøÉÂÜÖÂÆπÊÄªÁªìÔºà50Â≠ó‰ª•ÂÜÖÔºâ"
}}

Á´†ËäÇÂÜÖÂÆπÔºö
{content}
""",
            
            'volume_summary': """
ËØ∑Âü∫‰∫é‰ª•‰∏ãÁ´†ËäÇÂàÜÊûêÔºåÊÄªÁªìÁ¨¨{volume_id}Âç∑ÁöÑÊï¥‰ΩìÂÜÖÂÆπÔºö

Âç∑Ê†áÈ¢òÔºö{volume_title}
Á´†ËäÇÊï∞Ôºö{chapter_count}
ÊÄªÂ≠óÊï∞Ôºö{total_words}
Á´†ËäÇÂàÜÊûêÔºö{chapter_analyses}

ËØ∑ËøîÂõûJSONÊ†ºÂºèÁöÑÂç∑Á∫ßÊÄªÁªìÔºö
{{
  "volume_themes": ["Âç∑Á∫ß‰∏ªÈ¢ò1", "Âç∑Á∫ß‰∏ªÈ¢ò2"],
  "main_characters": [
    {{
      "name": "‰∏ªË¶ÅËßíËâ≤",
      "role": "ËßíËâ≤ÂÆö‰Ωç",
      "arc_summary": "Âú®Êú¨Âç∑ÁöÑÂèëÂ±ïËΩ®Ëøπ"
    }}
  ],
  "plot_arc": "Êï¥Âç∑ÊÉÖËäÇÂèëÂ±ïÊ¶ÇËø∞",
  "key_events": ["ÂÖ≥ÈîÆ‰∫ã‰ª∂1", "ÂÖ≥ÈîÆ‰∫ã‰ª∂2"],
  "climax_chapters": ["È´òÊΩÆÁ´†ËäÇ"],
  "character_development": "‰∏ªË¶ÅËßíËâ≤ÂèëÂ±ïÂèòÂåñ",
  "volume_conclusion": "Âç∑ÁªìËÆ∫ÊàñÊÇ¨Âøµ",
  "connection_to_next": "‰∏é‰∏ã‰∏ÄÂç∑ÁöÑËøûÊé•ÁÇπ"
}}
"""
        }
    
    async def process_complete_5_volumes(self) -> Dict[str, Any]:
        """Process the first 5 volumes from complete dataset"""
        
        try:
            # Step 1: Get complete dataset and parse structure
            print("\nüìñ Step 1: Fetching complete dataset and parsing structure...")
            structure_result = await self._parse_complete_novel_structure()
            
            if not structure_result['success']:
                raise Exception(f"Structure parsing failed: {structure_result['error']}")
            
            parsed_structure = structure_result['parsed_structure']
            all_chapters = structure_result['all_chapters']
            
            print(f"‚úÖ Complete structure parsed successfully:")
            print(f"   üìö Total volumes available: {len(parsed_structure)}")
            print(f"   üìÑ Total chapters available: {len(all_chapters)}")
            print(f"   üìù Total content: {structure_result['total_content_length']:,} characters")
            print(f"   üéØ Processing first 5 volumes...")
            
            # Step 2: Identify first 5 volumes (excluding volume 0 if exists)
            volumes_to_process = []
            for vol_id in sorted(parsed_structure.keys()):
                if vol_id > 0 and len(volumes_to_process) < 5:
                    volumes_to_process.append(vol_id)
            
            print(f"\nüéØ Target volumes: {volumes_to_process}")
            
            # Verify we have 5 volumes
            if len(volumes_to_process) < 5:
                print(f"‚ö†Ô∏è  Warning: Only {len(volumes_to_process)} volumes available")
            
            # Step 3: Process each of the first 5 volumes
            for i, volume_id in enumerate(volumes_to_process, 1):
                print(f"\n" + "=" * 60)
                print(f"üìö PROCESSING VOLUME {volume_id} ({i}/{len(volumes_to_process)})")
                print("=" * 60)
                
                await self._process_single_volume(volume_id, parsed_structure[volume_id])
                
                # Progress update
                self._print_progress_update(i, len(volumes_to_process))
            
            # Step 4: Generate final comprehensive report
            print(f"\nüìä Generating comprehensive final report...")
            final_report = self._generate_final_report()
            
            return final_report
            
        except Exception as e:
            logger.error(f"Complete volume processing failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _parse_complete_novel_structure(self) -> Dict[str, Any]:
        """Parse complete novel structure from all Qdrant data"""
        try:
            from qdrant_client import QdrantClient
            client = QdrantClient(url="http://localhost:32768", verify=False)
            
            # Get ALL data points
            print("   üì• Fetching all data points...")
            all_points = []
            offset = None
            batch_size = 1000
            
            while True:
                points = client.scroll(
                    collection_name="test_novel2",
                    limit=batch_size,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                batch_points = points[0]
                all_points.extend(batch_points)
                
                print(f"      Retrieved batch: {len(batch_points)} points (total: {len(all_points)})")
                
                if len(batch_points) < batch_size:
                    break
                
                offset = points[1] if len(points) > 1 and points[1] else None
                if not offset:
                    break
            
            print(f"   ‚úÖ Total data points retrieved: {len(all_points)}")
            
            # Combine all content
            combined_content = ""
            for point in all_points:
                if 'chunk' in point.payload:
                    combined_content += point.payload['chunk'] + "\n\n"
            
            print(f"   üìù Combined content length: {len(combined_content):,} characters")
            
            # Parse with hierarchical parser
            print("   üîß Running hierarchical parser...")
            chapters = self.parser.parse_content_hierarchy(combined_content)
            
            print(f"   ‚úÖ Parsing complete: {len(self.parser.parsed_structure)} volumes, {len(chapters)} chapters")
            
            return {
                'success': True,
                'parsed_structure': self.parser.parsed_structure,
                'all_chapters': chapters,
                'total_content_length': len(combined_content),
                'total_data_points': len(all_points)
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _process_single_volume(self, volume_id: int, volume_structure):
        """Process a single volume with detailed monitoring"""
        
        volume_start_time = datetime.now()
        volume_stats = VolumeProcessingStats(
            volume_id=volume_id,
            volume_title=volume_structure.volume_title,
            chapters_count=len(volume_structure.chapters),
            start_time=volume_start_time
        )
        
        print(f"üìñ Volume {volume_id}: {volume_structure.volume_title}")
        print(f"üìÑ Chapters to process: {len(volume_structure.chapters)}")
        print(f"üìù Total words in volume: {sum(ch.word_count for ch in volume_structure.chapters):,}")
        print(f"üïê Start time: {volume_start_time.strftime('%H:%M:%S')}")
        
        chapter_analyses = []
        
        # Process each chapter
        for chapter_num, chapter in enumerate(volume_structure.chapters, 1):
            print(f"\n  üîÑ Processing Chapter {chapter.chapter_id} ({chapter_num}/{len(volume_structure.chapters)})...")
            print(f"      üìù Content length: {len(chapter.content):,} chars, Words: {chapter.word_count:,}")
            
            chapter_start = time.time()
            
            try:
                # Analyze chapter with DeepSeek
                analysis_result = await self._analyze_chapter_with_deepseek(
                    volume_id, chapter
                )
                
                if analysis_result['success']:
                    chapter_analyses.append(analysis_result['analysis'])
                    volume_stats.chapters_successful += 1
                    volume_stats.total_tokens += analysis_result['tokens_used']
                    volume_stats.total_cost += analysis_result['cost']
                    volume_stats.deepseek_calls += 1
                    
                    chapter_time = time.time() - chapter_start
                    print(f"    ‚úÖ Chapter {chapter.chapter_id}: {analysis_result['tokens_used']} tokens, {chapter_time:.1f}s")
                    print(f"       üí∞ Cost: ${analysis_result['cost']:.4f}")
                    
                    # Show brief analysis preview
                    if 'summary' in analysis_result['analysis']:
                        summary = analysis_result['analysis']['summary'][:60] + "..." if len(analysis_result['analysis']['summary']) > 60 else analysis_result['analysis']['summary']
                        print(f"       üìã Summary: {summary}")
                    
                else:
                    volume_stats.errors.append(f"Chapter {chapter.chapter_id}: {analysis_result['error']}")
                    print(f"    ‚ùå Chapter {chapter.chapter_id}: {analysis_result['error']}")
                
                volume_stats.chapters_processed += 1
                
                # Rate limiting - small delay between requests
                await asyncio.sleep(0.5)
                
            except Exception as e:
                error_msg = f"Chapter {chapter.chapter_id} error: {e}"
                volume_stats.errors.append(error_msg)
                print(f"    ‚ùå {error_msg}")
        
        # Generate volume summary
        print(f"\n  üîÑ Generating volume-level summary...")
        try:
            volume_summary = await self._generate_volume_summary(
                volume_id, volume_structure, chapter_analyses
            )
            
            if volume_summary['success']:
                volume_stats.total_tokens += volume_summary['tokens_used']
                volume_stats.total_cost += volume_summary['cost']
                volume_stats.deepseek_calls += 1
                print(f"    ‚úÖ Volume summary: {volume_summary['tokens_used']} tokens")
                
                # Show brief summary preview
                if 'summary' in volume_summary and 'plot_arc' in volume_summary['summary']:
                    plot_preview = volume_summary['summary']['plot_arc'][:80] + "..." if len(volume_summary['summary']['plot_arc']) > 80 else volume_summary['summary']['plot_arc']
                    print(f"       üìñ Plot arc: {plot_preview}")
            
        except Exception as e:
            volume_stats.errors.append(f"Volume summary error: {e}")
            print(f"    ‚ùå Volume summary error: {e}")
        
        # Finalize volume stats
        volume_stats.end_time = datetime.now()
        volume_stats.processing_time = (volume_stats.end_time - volume_stats.start_time).total_seconds()
        
        # Update global stats
        self.global_stats['volumes_stats'].append(volume_stats)
        self.global_stats['total_tokens'] += volume_stats.total_tokens
        self.global_stats['total_cost'] += volume_stats.total_cost
        self.global_stats['total_deepseek_calls'] += volume_stats.deepseek_calls
        self.global_stats['total_processing_time'] += volume_stats.processing_time
        
        # Print volume completion summary
        success_rate = (volume_stats.chapters_successful / volume_stats.chapters_count) * 100
        print(f"\n‚úÖ Volume {volume_id} completed:")
        print(f"   ‚è±Ô∏è  Processing time: {volume_stats.processing_time:.1f} seconds")
        print(f"   üìä Success rate: {volume_stats.chapters_successful}/{volume_stats.chapters_count} ({success_rate:.1f}%)")
        print(f"   üí∞ Tokens used: {volume_stats.total_tokens:,}")
        print(f"   üíµ Cost: ${volume_stats.total_cost:.4f}")
        print(f"   üîß DeepSeek calls: {volume_stats.deepseek_calls}")
        
        if volume_stats.errors:
            print(f"   ‚ö†Ô∏è  Errors: {len(volume_stats.errors)}")
    
    async def _analyze_chapter_with_deepseek(self, volume_id: int, chapter) -> Dict[str, Any]:
        """Analyze a chapter using DeepSeek with token tracking"""
        
        try:
            # Prepare content (limit to reasonable size for analysis)
            content_for_analysis = chapter.content[:2000] if len(chapter.content) > 2000 else chapter.content
            
            # Create prompt
            prompt = self.analysis_prompts['chapter_analysis'].format(
                volume_id=volume_id,
                chapter_id=chapter.chapter_id,
                chapter_title=chapter.chapter_title,
                word_count=chapter.word_count,
                content=content_for_analysis
            )
            
            # Query DeepSeek
            messages = [{"role": "user", "content": prompt}]
            response = await self.deepseek_client.generate_character_response(
                messages=messages,
                max_tokens=1200,
                temperature=0.3
            )
            
            if response.get("success"):
                # Calculate tokens and cost
                tokens_used = len(prompt) // 4 + len(response["response"]) // 4
                cost = tokens_used * 0.00002  # Rough estimate
                
                # Try to parse JSON
                try:
                    # Extract JSON from markdown if present
                    response_text = response["response"]
                    if "```json" in response_text:
                        start = response_text.find("```json") + 7
                        end = response_text.find("```", start)
                        json_text = response_text[start:end].strip()
                    elif "{" in response_text:
                        start = response_text.find("{")
                        end = response_text.rfind("}") + 1
                        json_text = response_text[start:end]
                    else:
                        json_text = response_text
                    
                    parsed_analysis = json.loads(json_text)
                    
                except json.JSONDecodeError:
                    parsed_analysis = {"raw_response": response["response"]}
                
                return {
                    'success': True,
                    'analysis': parsed_analysis,
                    'tokens_used': tokens_used,
                    'cost': cost,
                    'raw_response': response["response"]
                }
            else:
                return {
                    'success': False,
                    'error': response.get('error', 'Unknown DeepSeek error'),
                    'tokens_used': 0,
                    'cost': 0.0
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'tokens_used': 0,
                'cost': 0.0
            }
    
    async def _generate_volume_summary(self, volume_id: int, volume_structure, chapter_analyses: List[Dict]) -> Dict[str, Any]:
        """Generate volume-level summary"""
        
        try:
            # Prepare chapter analyses summary (limit size)
            analyses_summary = json.dumps(chapter_analyses[:5], ensure_ascii=False)
            total_words = sum(ch.word_count for ch in volume_structure.chapters)
            
            prompt = self.analysis_prompts['volume_summary'].format(
                volume_id=volume_id,
                volume_title=volume_structure.volume_title,
                chapter_count=len(volume_structure.chapters),
                total_words=total_words,
                chapter_analyses=analyses_summary
            )
            
            messages = [{"role": "user", "content": prompt}]
            response = await self.deepseek_client.generate_character_response(
                messages=messages,
                max_tokens=1000,
                temperature=0.3
            )
            
            if response.get("success"):
                tokens_used = len(prompt) // 4 + len(response["response"]) // 4
                cost = tokens_used * 0.00002
                
                # Try to parse JSON
                try:
                    response_text = response["response"]
                    if "```json" in response_text:
                        start = response_text.find("```json") + 7
                        end = response_text.find("```", start)
                        json_text = response_text[start:end].strip()
                    elif "{" in response_text:
                        start = response_text.find("{")
                        end = response_text.rfind("}") + 1
                        json_text = response_text[start:end]
                    else:
                        json_text = response_text
                    
                    parsed_summary = json.loads(json_text)
                    
                except json.JSONDecodeError:
                    parsed_summary = {"raw_response": response["response"]}
                
                return {
                    'success': True,
                    'summary': parsed_summary,
                    'tokens_used': tokens_used,
                    'cost': cost
                }
            else:
                return {
                    'success': False,
                    'error': response.get('error', 'Unknown error'),
                    'tokens_used': 0,
                    'cost': 0.0
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'tokens_used': 0,
                'cost': 0.0
            }
    
    def _print_progress_update(self, completed: int, total: int):
        """Print progress update"""
        
        progress_percent = (completed / total) * 100
        elapsed_time = (datetime.now() - self.global_stats['start_time']).total_seconds()
        estimated_total_time = elapsed_time * total / completed
        remaining_time = estimated_total_time - elapsed_time
        
        print(f"\n" + "üîÑ" * 20 + " PROGRESS UPDATE " + "üîÑ" * 20)
        print(f"üìä Progress: {completed}/{total} volumes ({progress_percent:.1f}%)")
        print(f"‚è±Ô∏è  Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
        print(f"‚è≥ Estimated remaining: {remaining_time:.1f} seconds ({remaining_time/60:.1f} minutes)")
        print(f"üí∞ Total tokens so far: {self.global_stats['total_tokens']:,}")
        print(f"üíµ Total cost so far: ${self.global_stats['total_cost']:.4f}")
        print(f"üîß Total API calls: {self.global_stats['total_deepseek_calls']}")
        print("üîÑ" * 56)
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """Generate comprehensive final report"""
        
        end_time = datetime.now()
        total_time = (end_time - self.global_stats['start_time']).total_seconds()
        
        # Calculate aggregate statistics
        total_chapters = sum(vol.chapters_count for vol in self.global_stats['volumes_stats'])
        successful_chapters = sum(vol.chapters_successful for vol in self.global_stats['volumes_stats'])
        total_errors = sum(len(vol.errors) for vol in self.global_stats['volumes_stats'])
        
        report = {
            'processing_metadata': {
                'start_time': self.global_stats['start_time'].isoformat(),
                'end_time': end_time.isoformat(),
                'total_processing_time': f"{total_time:.1f} seconds",
                'total_processing_minutes': f"{total_time/60:.1f} minutes",
                'volumes_processed': len(self.global_stats['volumes_stats']),
                'chapters_processed': total_chapters,
                'successful_chapters': successful_chapters,
                'success_rate': f"{successful_chapters/max(total_chapters, 1)*100:.1f}%"
            },
            'token_and_cost_analysis': {
                'total_tokens_used': self.global_stats['total_tokens'],
                'total_cost': f"${self.global_stats['total_cost']:.4f}",
                'total_deepseek_calls': self.global_stats['total_deepseek_calls'],
                'average_tokens_per_chapter': self.global_stats['total_tokens'] // max(successful_chapters, 1),
                'average_cost_per_chapter': f"${self.global_stats['total_cost']/max(successful_chapters, 1):.4f}",
                'average_tokens_per_volume': self.global_stats['total_tokens'] // max(len(self.global_stats['volumes_stats']), 1),
                'tokens_per_second': self.global_stats['total_tokens'] / max(total_time, 1)
            },
            'volume_breakdown': [
                {
                    'volume_id': vol.volume_id,
                    'volume_title': vol.volume_title,
                    'chapters_count': vol.chapters_count,
                    'chapters_successful': vol.chapters_successful,
                    'processing_time': f"{vol.processing_time:.1f}s",
                    'processing_minutes': f"{vol.processing_time/60:.1f}m",
                    'tokens_used': vol.total_tokens,
                    'cost': f"${vol.total_cost:.4f}",
                    'deepseek_calls': vol.deepseek_calls,
                    'errors_count': len(vol.errors),
                    'success_rate': f"{vol.chapters_successful/max(vol.chapters_count, 1)*100:.1f}%"
                }
                for vol in self.global_stats['volumes_stats']
            ],
            'performance_metrics': {
                'chapters_per_minute': (successful_chapters / max(total_time, 1)) * 60,
                'volumes_per_hour': (len(self.global_stats['volumes_stats']) / max(total_time, 1)) * 3600,
                'average_processing_time_per_volume': total_time / max(len(self.global_stats['volumes_stats']), 1),
                'total_errors': total_errors,
                'error_rate': f"{total_errors/max(total_chapters, 1)*100:.1f}%"
            },
            'success': total_errors == 0
        }
        
        return report
    
    def print_final_report(self, report: Dict[str, Any]):
        """Print comprehensive final report"""
        
        print("\n" + "=" * 80)
        print("üìä FINAL 5-VOLUME PROCESSING REPORT (COMPLETE DATASET)")
        print("=" * 80)
        
        # Processing metadata
        meta = report['processing_metadata']
        print(f"\n‚è±Ô∏è  PROCESSING SUMMARY:")
        print(f"   üïê Start: {meta['start_time']}")
        print(f"   üïê End: {meta['end_time']}")
        print(f"   ‚è±Ô∏è  Total time: {meta['total_processing_time']} ({meta['total_processing_minutes']})")
        print(f"   üìö Volumes processed: {meta['volumes_processed']}/5")
        print(f"   üìÑ Chapters processed: {meta['chapters_processed']}")
        print(f"   ‚úÖ Success rate: {meta['success_rate']}")
        
        # Token and cost analysis
        tokens = report['token_and_cost_analysis']
        print(f"\nüí∞ TOKEN USAGE & COSTS:")
        print(f"   üî¢ Total tokens: {tokens['total_tokens_used']:,}")
        print(f"   üíµ Total cost: {tokens['total_cost']}")
        print(f"   üìû API calls: {tokens['total_deepseek_calls']}")
        print(f"   üìä Avg tokens/chapter: {tokens['average_tokens_per_chapter']}")
        print(f"   üí≥ Avg cost/chapter: {tokens['average_cost_per_chapter']}")
        print(f"   üìñ Avg tokens/volume: {tokens['average_tokens_per_volume']}")
        print(f"   ‚ö° Tokens/second: {tokens['tokens_per_second']:.1f}")
        
        # Volume breakdown
        print(f"\nüìö VOLUME-BY-VOLUME BREAKDOWN:")
        for vol in report['volume_breakdown']:
            print(f"\n   üìñ Volume {vol['volume_id']}: {vol['volume_title']}")
            print(f"      üìÑ Chapters: {vol['chapters_successful']}/{vol['chapters_count']} ({vol['success_rate']})")
            print(f"      ‚è±Ô∏è  Time: {vol['processing_time']} ({vol['processing_minutes']})")
            print(f"      üí∞ Tokens: {vol['tokens_used']:,} | Cost: {vol['cost']}")
            print(f"      üìû API calls: {vol['deepseek_calls']}")
            if vol['errors_count'] > 0:
                print(f"      ‚ö†Ô∏è  Errors: {vol['errors_count']}")
        
        # Performance metrics
        perf = report['performance_metrics']
        print(f"\n‚ö° PERFORMANCE METRICS:")
        print(f"   üìä Chapters/minute: {perf['chapters_per_minute']:.1f}")
        print(f"   üìö Volumes/hour: {perf['volumes_per_hour']:.1f}")
        print(f"   ‚è±Ô∏è  Avg time/volume: {perf['average_processing_time_per_volume']:.1f}s")
        print(f"   ‚ùå Total errors: {perf['total_errors']}")
        print(f"   üìâ Error rate: {perf['error_rate']}")
        
        # Final status
        status = "‚úÖ SUCCESS" if report['success'] else "‚ö†Ô∏è  COMPLETED WITH ERRORS"
        print(f"\nüèÅ FINAL STATUS: {status}")
        print("=" * 80)

async def main():
    """Main execution function"""
    
    processor = Complete5VolumeProcessor()
    
    try:
        # Process 5 volumes from complete dataset
        report = await processor.process_complete_5_volumes()
        
        # Print comprehensive report
        processor.print_final_report(report)
        
        # Save report to file
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"complete_5_volume_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            # Convert dataclass objects to dicts for JSON serialization
            json_report = json.loads(json.dumps(report, default=str, ensure_ascii=False))
            json.dump(json_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Detailed report saved to: {report_file}")
        
    except Exception as e:
        print(f"‚ùå Processing failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # Cleanup
        if processor.deepseek_client.session:
            await processor.deepseek_client.close()

if __name__ == "__main__":
    asyncio.run(main())